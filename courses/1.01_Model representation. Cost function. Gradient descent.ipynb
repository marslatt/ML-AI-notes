{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ba449e",
   "metadata": {},
   "source": [
    "## Model Representation\n",
    "\n",
    "The simplest model example is the univariate (simple) linear regression, representing a straight line: $f_{w,b}$(x) = $w$x + $b$\n",
    "<br>\n",
    "* $f_{w,b}$(x) = hypothesis or model = a function that takes as input the value of $x$ and outputs the corresponding estimated value of $\\hat{y}$ (i.e. maps from x's to y's).\n",
    "* $w, b$ = weights or coefficients or parameters of the model (adjusted as the model learns from data)\n",
    "* $m$ = training examples = number of rows \n",
    "* $x$ = input variables or features (arranged in columns)  \n",
    "* $y$ = output or target variable to be predicted (the true output in the labeled data)\n",
    "* $\\hat{y}$ = predicted output (may be or may not be the true output value)\n",
    "* $(x, y)$ or $(x^{(i)}, y^{(i)})$ = a single training example (the i-th training example) = a single row in a data table "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3b963",
   "metadata": {},
   "source": [
    "## Cost function \n",
    "\n",
    "Cost (or error or loss) is a measure of how accurately the model performs on training data. A commonly used cost function for linear regression is the \"square difference\" function $J(w,b)$: \n",
    "<br><br>$J(w,b)= \\frac{1}{2m} \\sum_{i=1}^{m} ( \\hat{y}^{(i)} - y^{(i)} )^{2} = \\frac{1}{2m} \\sum_{i=1}^{m} ( f_{w,b}(x^{(i)}) - y^{(i)} )^{2} $<br> \n",
    "* The \"square difference\" cost is a reasonable choice and works well for most regression problems. While there are other alternative cost functions that also work well, this one is probably the most commonly used.\n",
    "* Cost function let us figure out how to fit the best possible model to our data, in this case by minimizing the square difference between actual and expected outputs to provide optimal choices for $w$ and $b$. \n",
    "* The fact that the $J(w,b)$ squares the loss, ensures that the 'error surface' is convex and the function increases rapidly as $w$ gets too small or too big.  \n",
    "* It always has a minimum which can be reached by following the gradients in all dimensions. \n",
    "* The algorithm used to minimize $J(w,b)$  is called \"Gradient Descent\" (it's used in many models, not only in regression). \n",
    "* Оn а 2D plot $J(w,b)$ looks like a parabole, while in 3D it resembles a \"hammock\" and can also be depicted as a countour plot of concentric elipses, where all points with the same $J$ form elipses and $min$ $J$ is in its center.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7feb54b",
   "metadata": {},
   "source": [
    "## Gradient and dervative\n",
    "\n",
    "* In single-variable calculus, a derivative of a function shows how fast the function is changing at a particular point. More specifically, it gives the slope of the tangent line at that point.\n",
    "* A positive slope rises from left to right, while a negative slope falls from lef to right. A horizontal slope is defined as zero, while a vertical slope is unfdefined.\n",
    "* If a function depends on more than one variable, a gradient is used instead of a single derivative. The gradient represents a vector (a list) of partial derivatives — one for each variable. \n",
    "* At each point of the function, the gradient shows the direction of the fastest increase of the function, and how steep that increase is in each direction.\n",
    "* To find the minima of the function, starting from an arbitratry point, the direction of the negative gradient can be used, as it is opposite to the fastest increase.\n",
    "* The algoritm that find the the minima of a function, by iteratively choosing the direction of the negative gradient at each step, is called Gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da957b21-d617-40a7-9546-c65e40f0fdaa",
   "metadata": {},
   "source": [
    "## Gradient descent (GD)\n",
    "\n",
    "Repeat { $θ_{j}$ := $θ_{j}$ - $\\alpha$ $\\frac{\\partial}{\\partial θ_{j}}$J($θ$)  }\n",
    "\n",
    "* GD involves repeated steps to update the values of the parameters $w$ and $b$, while iteratively minimizing the cost function $ J(w,b) $\n",
    "* At each point, the negative gradient (opposite to the gradient) shows in which direction to take \"baby steps down the hill\". Due to the \"bowl shape\" of the cost function, following the direction of the negative gradient will always lead GD towards the bottom, where the graient is 0. The direction of the steepest descent should be taken at each step during this process.\n",
    "* Cost should always decrease in successful runs. It starts large and rapidly declines in the beginning. The partial derivatives $ \\frac{d}{dw} J(w,b) $ and $ \\frac{d}{db} J(w,b) $ also get smaller, rapidly at first and then more slowy. As the process nears \"the bottom of the bowl\" progress is slower, due to the smaller value of derivatives at that point.\n",
    "* How to update $w$? By using the partial derivative of $J(w,b)$ with respect to $w$. \n",
    "<br><br>\n",
    "$ w = w - \\alpha \\frac{d}{dw} J(w,b) $ &emsp;[1]\n",
    "<br><br>\n",
    "* Here $\\alpha$ is the so called <b>learning rate</b> (a parameter showing how big of a step to take). It controls how quickly we descend the slope of $J(w,b)$. Its value is typically a very small positive number between 0 and 1. \n",
    "* How to update $b$? By using the partial derivative of $J(w,b)$ with respect to $b$.<br><br>\n",
    "$ b = b - \\alpha \\frac{d}{db} J(w,b) $ &emsp;[2]\n",
    "<br><br> \n",
    "* We must repeat updating $w$ and $b$ until convergence - i.e. until reaching such a minima where $w$ and $b$ no longer change much with each additional step. Note that we <b>simultaneously</b> update both $w$ and $b$ to reach convergence (by using the \"old\" values from the previous step to calculate both).\n",
    "* How to calculate  partial derivatives for the update terms of $w$ and $b$? \n",
    "<br><br>\n",
    "$ \\frac{d}{dw} J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} ( f_{w,b}(x^{(i)}) - y^{(i)} ) x^{(i)}  $ &emsp; for [1]\n",
    "<br><br>\n",
    "$ \\frac{d}{db} J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} ( f_{w,b}(x^{(i)}) - y^{(i)} )  $ &emsp;for [2]\n",
    "<br><br>\n",
    "* Note: Here is why we use $\\frac{1}{2}$ in the cost function - because it makes our partial derivative calculations neater.<br>\n",
    "* Note: The square error cost in linear regression does not have the problem of finding a $J(w,b)$ minima - as only one minima exists for it. But with neural networks the cost function may not be in a \"bowl shape\" i.e. be a beautiful convex function. When training a nural network, we can have a more complex cost function, that has more than one minima. Depending on the starting point and the steps we take, we can find different \"local minima\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e28cb-99fb-4936-aefe-a188c2205e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
