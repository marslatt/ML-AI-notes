{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4823a15",
   "metadata": {},
   "source": [
    "## Simple Linear Regression \n",
    "\n",
    "Simple linear regression model fits a straight line to the data: $f_{w,b}$(x) = $w$x + $b$\n",
    "\n",
    "* A <b>supervised</b> learning model (meaning that it learns from labeled data) for predicting a <b>continuous valued output</b> - such as the price of houses (real numbers i.e. scalar values).\n",
    "* Types of Linear regression: \n",
    "** Univariate (simple) linear regression uses one independent variable to predict the output $\\hat{y}$ (y_hat). \n",
    "** Multiple linear regression uses two or more independent variables to predict $\\hat{y}$. Using more variables allows the model to account for more factors that influence y and generally improve predictive accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db06bf7-c9d1-4857-a6dd-da6aa9f78310",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression \n",
    "\n",
    "Model representation - a straight line: $f_{\\vec{w},b}(\\vec{x}) = \\vec{w}.\\vec{X}$ + $b$ ,   where $.$ is a dot product \n",
    "<br>\n",
    "> Multiple Linear Regression is a model that estimates the relationship between a quantitative dependent variable and two or more independent variables using a straight line\n",
    "<br>\n",
    "\n",
    "* $w_{1} ... w_{n}, b = $ weights or coefficients or parameters of the model (adjusted as the model learns from data)\n",
    "* $\\vec{w} = [w_{1} ... w_{n}] = n$-length vector\n",
    "* $b = $ scalar \n",
    "* $\\vec{X} = $ feature matrix with $m$ rows and $n$ columns\n",
    "* $n = $ length of sample vector\n",
    "* $m = $ number of training samples\n",
    "* $x^{(i)} = (x^{(i)}_{1}, ... , x^{(i)}_{n}) = $ feature vector $i$\n",
    "* $x^{(i)}_{j} = $ element $j$ in sample $i$\n",
    "* $f_{\\vec{w},b}(x^{(i)}) = w_{1}x_{1}$ + ... + $w_{n}x_{1}$ + $b$\n",
    "* $J(\\vec{w},b) = J(w_{1}...w_{n},b) = \\frac{1}{2m} \\sum_{i=1}^{m} ( f_{\\vec{w},b}(x^{(i)}) - y^{(i)} )^{2} = $ cost function, where $f_{\\vec{w},b}(\\vec{x}) = \\vec{w}.x^{(i)}$ + $b$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b0d0f-36df-438e-be16-bad32d410d29",
   "metadata": {},
   "source": [
    "## Gradient Descent with multiple variables\n",
    "\n",
    "Repeat until convergence: {\n",
    "    <br>  $w_{j} = w_{j} - \\alpha \\frac{\\partial}{\\partial w_{j}}J(\\vec{w},b)$ \n",
    "    <br>  $b = b - \\alpha \\frac{\\partial}{\\partial b}J(\\vec{w},b)$ <br> \n",
    "}\n",
    "<br><br>\n",
    "where: \n",
    "<br><br>$\\frac{\\partial}{\\partial w_{j}}J(\\vec{w},b) = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} ( f_{\\vec{w},b}(x^{(i)}) - y^{(i)} )x^{(i)}_{j}$ \n",
    "<br><br>$\\frac{\\partial}{\\partial b}J(\\vec{w},b) = \\frac{1}{m} \\sum_{i=1}^{m} ( f_{\\vec{w},b}(x^{(i)}) - y^{(i)} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57486bcd-da53-4894-8124-b77e818b188a",
   "metadata": {},
   "source": [
    "## Alternative to Gradient Descent:\n",
    " \n",
    "> For models like Linear Regression, we can use two types of techniques to fit the parameter: Normal Equation and Gradient descent. \n",
    "\n",
    "* Normal equation is used only for linear regression i.e. does not generalize for other learning algorithms\n",
    "* It's slow for number of features > 10000\n",
    "* May be used on the background by ML libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac31ac96-6cef-458a-918d-4317e3832e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
