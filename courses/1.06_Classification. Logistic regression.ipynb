{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f4e3a47-5274-4503-8f3a-05e7f2a8c38e",
   "metadata": {},
   "source": [
    "## Logistic regression model\n",
    "\n",
    "* The outpyt $y$ can take only a small handful of possible values i.e. several \"classes\" or \"categories\".\n",
    "* Fits an an S-shaped curve $g(z)$ to the data, called sigmoid function or logistic regression function.\n",
    "* The output of the sigmoid function is always between 0 and 1.\n",
    "* $g(z)$ = <font size=4>$\\frac{1}{1 + e^{-z}}$</font>, where <font size=2>$0<g(z)<1; e\\approx2.7$</font>\n",
    "<br><br> picture of sigmoid <br><br>\n",
    "\n",
    "\n",
    "* How to model $g(z)$ with  $f_{\\vec{w},b}(\\vec{x}) = \\vec{w}.\\vec{x} + b$ ?\n",
    "* * Substitute $z$ in $g(z)$ with the same old $f_{\\vec{w},b}(\\vec{x})$, already familiar from regression problems.\n",
    "* * Keep on modeling $f_{\\vec{w},b}(\\vec{x})$ with regression gradient descent and cost functions.\n",
    "* * As a result, we can again take as input data the same set of features $\\vec{x}$, but output a number between 0 and 1 for every $x$.\n",
    "* Think of the output of the logistic regression as a \"probability\" for the label $y$ to be true or false (i.e. equal to 1 or 0), given a certain input of $x$.\n",
    "* Example:\n",
    "* * Let $\\vec{x}$ be a feature vector denoting tumor size.\n",
    "* * Let $y$ be a label (class) denoting malignancy: 1 if the tumor is malignant, and 0 if not.\n",
    "* * When we get a prediction value of $y=0.7$ - it means 70% chance for $y$ to be \"true\" (i.e. equal to 1) and therfore for the tumor to be malignant.\n",
    "<br><br> picture of sigmoid from the example here <br><br>\n",
    "\n",
    "\n",
    "* A valid notation for logistic regression is also: $f_{\\vec{w},b}(\\vec{x}) = P(y=1 \\mid \\vec{x};\\vec{w},b)$, where $P(y=1) + P(y=0) = 1$\n",
    "* * This litteraly means \"probability that $y$ is 1, given input $\\vec{x}$ and parameters $\\vec{w},b$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db23ed2",
   "metadata": {},
   "source": [
    "## Decision boundary of logistic regression\n",
    "\n",
    "* The decision boundary is the line given by the equation: $z=\\vec{w}.\\vec{x}+b=0$\n",
    "* The model predicts \"true\" when $\\vec{w}.\\vec{x}+b \\geq 0$\n",
    "* Example <b>I</b>:\n",
    "* * $f_{\\vec{w},b}(\\vec{x}) = g(z) = g(w_1x_1 + w_2x_2 + b)$, where $\\vec{w}=(1, 1), b=-3$\n",
    "* * $z = \\vec{w}.\\vec{x}+b = x_1 + x_2 -3 = 0$\n",
    "* * The decision boundry is given by the srtaight line $z: x_1+x_2=3$\n",
    "<br><br> picture of decision boundary from the example 1 here <br><br>\n",
    "\n",
    "* The decision boundary can also be different from a straight line.\n",
    "* With a higher-order polynomial terms in $z$, the decision boundary can be a quite complex curve i.e. the logistic regression can fit pretty complex data.\n",
    "* Example <b>II</b>:\n",
    "* * $f_{\\vec{w},b}(\\vec{x}) = g(z) = g(w_1x_1^2 + w_2x_2^2 + b)$, where $\\vec{w}=(1, 1), b=-1$\n",
    "* * $z = \\vec{w}.\\vec{x}+b = x_1^2 + x_2^2 -1 = 0$\n",
    "* * The decision boundry is given by the circle $z: x_1^2+x_2^2=1$\n",
    "<br><br> picture of decision boundary from the example 2 here <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a004727-6079-422c-badd-18a1b793e830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438d205-6b27-4abe-b6ef-e3bd81dcccd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9775ed90-7e36-4fb5-8f69-e3c8c59d6951",
   "metadata": {},
   "source": [
    "How to Do Classification with Scikit-Learn \n",
    "\n",
    "You can use scikit-learn to perform classification using any of its numerous classification algorithms (also known as classifiers), including: \n",
    "\n",
    "Decision Tree/Random Forest – the Decision Tree classifier has dataset attributes classed as nodes or branches in a tree. The Random Forest classifier is a meta-estimator that fits a forest of decision trees and uses averages to improve prediction accuracy.\n",
    "\n",
    "K-Nearest Neighbors (KNN) – a simple classification algorithm, where K refers to the square root of the number of training records.\n",
    "\n",
    "Linear Discriminant Analysis – estimates the probability of a new set of inputs for every class.\n",
    "\n",
    "Logistic Regression – a model with an input variable (x) and an output variable (y), which is a discrete value of either 1 (yes) or 0 (no).\n",
    "\n",
    "Naive Bayes – a family of classifiers based on a simple Bayesian model that is comparatively fast and accurate. Bayesian theory explores the relationship between probability and possibility.\n",
    "\n",
    "Support Vector Machines (SVMs) – a model with associated learning algorithms that analyze data for classification. Also known as Support-Vector Networks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742dc82-e4a5-42f5-aec9-d6add9a8c249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
