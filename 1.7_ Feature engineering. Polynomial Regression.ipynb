{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5fd106",
   "metadata": {},
   "source": [
    "## Fefature engineering\n",
    "\n",
    "* Feature engineering means adding new features, engineered by using some of the existing ones of the model.\n",
    "* \"Using intuition to design new features, by transforming and combining original features.\"\n",
    "* Feature engineering allow us to fit also non-linear curves to our data.\n",
    "* Eample: $f_{\\vec{w},b}(\\vec{x})$ = $w_{1}x_{1}$ + $w_{2}x_{2}$ + $b$ \n",
    "<br>&emsp;&emsp;&emsp;&nbsp;&nbsp; $x_{3}$ = $x_{1}x_{2}$\n",
    "<br>&emsp;&emsp;&emsp;&nbsp;&nbsp; $f_{\\vec{w},b}(\\vec{x})$_new = $w_{1}x_{1}$ + $w_{2}x_{2}$ + $w_{3}x_{3}$ + $b$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a17d3a",
   "metadata": {},
   "source": [
    "## Polynomial regression \n",
    "\n",
    "Model representation: $f_{\\vec{w},b}(x)$ = $w_{1}x_{1}$ + $w_{2}x_{2}^{2}$ + $w_{3}x_{3}^{3}$ + ... + $w_{n}x_{n}^{n}$ + $b$  \n",
    " \n",
    "> Multiple Linear Regression is a form of regression analysis in which the relationship between an independent variable x and a quantitative dependent variable y is modeled as an nth degree polynomial.\n",
    "\n",
    "* Relies on feature engineering and the machinery of linear regression to fit complicated non-linear functions.\n",
    "* If you're using gradient descent, it's important to apply feature scaling to get your features into comparable ranges of values. \n",
    "* * Since the values grow exponentialy, we want to keep them small from the beginning to make gradient descent work faster.\n",
    "* Note that we can also use $\\sqrt{x}$, $x^{-\\frac{1}{3}}$ and etc., not only powers greater that 0.\n",
    "* * In this way the curve will never flatten, but will be much less steep.\n",
    "* When training, whatch out what parameters gradient descent is producing, especially if you are not sure what polynom would fit best.\n",
    "* * Gradient descent will emphasize the coefficients that helps most to fit the data. The longer you run the training - the more significant they will become, compared to the rest.\n",
    "* * Example: If you can add $x^{3}$ to a model that needs only $x^{2}$ &rarr; gradient descent will produce a very small coefficient for $x^{3}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca7fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
