{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616cc250",
   "metadata": {},
   "source": [
    "## Tidy vs Messy data\n",
    "\n",
    "> Long format (Tidy data) has become the standard for science and business because such data tables can easily be turned into graphs, analysis and insights. In a Tidy, or Long format data table, all the values from the same variable are in the same column, even if they were measured on different subjects or conditions. Each row therefore represents a single observation of all variables.\n",
    "* Most important rules when creating datasets:\n",
    "* * Columns = attributes (features, variables)\n",
    "* * Rows = observations (samples)\n",
    "* * Cells = values (one observation of one feature)\n",
    "* All other data is called messy data\n",
    "* A very good paper on tidy data: https://vita.had.co.nz/papers/tidy-data.pdf\n",
    "\n",
    "\n",
    "## Data Tidying steps\n",
    "\n",
    "* If features are distributed in multiple tables - merge the tables into one, add new columns if neccessary\n",
    "* Ensure that you've read the dataset correctly &rarr; Show the first 5 values (use ```data.head(5)```) \n",
    "* Check if the table header contains values &rarr; Add missing comlumn names (from the documentation)\n",
    "* Compare the number of variables and observations (```data.shape```) with the description/documentation \n",
    "* If a feature is distributed in multiple columns &rarr; Melt the columns that represent one and the same feature\n",
    "* If multiple variables are stored in one column &rarr; Identify and split the variables into separate columns\n",
    "* Treat missing values (nulls & NaNs): either remove them or replace them \n",
    "* Reindex if neccessary\n",
    "* Subset variables and observations\n",
    "* Summarize and group variables \n",
    "\n",
    "> ```Unpivoting``` is useful when you have a dataset in a ```wide``` format that you are looking to convert to a ```long``` format. Practically, this means that you have columns where you want to maintain their values, but turn them into rows. This is known as a ```melt``` in Pandas.\n",
    "\n",
    "> In the context of the Pandas library in Python, ```pivoting``` is a neat process that transforms a DataFrame into a new one by converting selected columns into new columns based on their values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92efed8",
   "metadata": {},
   "source": [
    "## Operations on Datasets - Basic tools\n",
    "\n",
    "<a href=\"https://github.com/pandas-dev/pandas/blob/main/doc/cheatsheet/Pandas_Cheat_Sheet.pdf\">Pandas cheat sheet</a>\n",
    "\n",
    "#### Subsetting Rows (selection)\n",
    "\n",
    "* First / last n records (observations):<br>\n",
    "``` data.head(10)```<br>\n",
    "``` data.tail() # 5 by default ```\n",
    "* Random n records:<br>\n",
    "``` data.sample(n = 10) ```<br> \n",
    "``` data.sample() # 1 random record by default ``` \n",
    "* Smallest / largest n records in a given column:<br>\n",
    "``` data.nsmallest(n, \"col_name\") ```<br> \n",
    "``` data.nlargest(n, \"col_name\") ``` \n",
    "* Subsetting by a Boolean expression (predicate):<br>\n",
    "* * Returns only rows where the expression returns True <br>\n",
    "``` data[data.col_name > 30] ``` \n",
    "\n",
    "#### Subsetting Columns (projection)\n",
    "\n",
    "* Single column (returns a ```Series``` object):<br>\n",
    "``` data[\"col_name\"] ``` <br>\n",
    "``` data.col_name # Possible in most cases ``` \n",
    "* More than one column (returns a DataFrame object):<br>\n",
    "``` data[[\"col_name1\", \"col_name2\"]] ``` \n",
    "* Combining filters:<br>\n",
    "``` data[data.col_name1 > \"2010-10-01\"][[\"col_name1\", \"col_name2\"]] ```<br> \n",
    "``` data.loc[data.col_name > \"2010-10-01\", [\"col_name1\", \"col_name2\"]] ``` \n",
    "* A note on Boolean expressions: <br>\n",
    "* * \"and\", \"or\", \"not\" are &, |, ~\n",
    "* * Always put parentheses around the individual expressions:<br> \n",
    "``` data[(data.col_name > \"2010-10-01\") & (data.col_name < \"2010-12-01\")]```\n",
    "\n",
    "#### Summary Statistics and Grouping\n",
    "\n",
    "* These methods work by columns. If multiple columns are passed, they are applied to each column individually:<br>\n",
    "``` data.col_name.count() # number of non-null values``` <br>\n",
    "``` data.col_name.min()```<br> \n",
    "``` data.col_name.max()``` <br>\n",
    "``` data.col_name.mean()``` <br>\n",
    "``` data.tcol_namemin.median()``` <br>\n",
    "``` data.col_name.std()``` \n",
    "* Summarize (describe) entire dataset: <br>\n",
    "``` data.describe()``` \n",
    "* Grouping:\n",
    "* * Splits the data into several groups based on the values of a column\n",
    "* * Apply a method after grouping or iterate over the groups (using a for-loop):<br>\n",
    "``` data.groupby(\"col_name\").mean()``` \n",
    "\n",
    "#### Further transformations\n",
    "\n",
    "* If needed, perform math operations: log, square root, addition, multiplication, etc.\n",
    "* * Be careful as you'll get results in different dimensions\n",
    "* Normalizing scores (such as using Z-scores) is recommended in most cases\n",
    "* * It's much better for ML algorithms to have data of similar scales\n",
    "* * You can do that manually or use a library (such as sklearn.preprocessing)\n",
    "* By convention, calculated columns are added to the dataset\n",
    "\n",
    "#### Describe all operations as you're doing them\n",
    "\n",
    "* Describe what you're doing and why\n",
    "* * Useful to check your work later (or allow others to do that)\n",
    "* * If needed, save the resulting dataset into a file\n",
    "* * Supply your data transformation log \n",
    "* * Provide a dataset description\n",
    "\n",
    "#### Outliers and Errors \n",
    "\n",
    "* Outliers – values which are far from their expected range or having a very low probability of happening (assuming a model)\n",
    "* Many possible cases\n",
    "* * Wrong data entry (e.g. an adult weighing 5kg might be 50kg or something else)\n",
    "* *  Wrong assumptions (the data is correct, our view isn't)\n",
    "* What to do?\n",
    "* * Inspect the data point and try to figure out what happened\n",
    "* * If needed, remove the row or try to replace the value\n",
    "* * Try a transformation\n",
    "* * If possible, perform analysis with and without the outlier(s) and compare your results\n",
    "\n",
    "##### Transformations on Features\n",
    " \n",
    "* The quality of our results depends strongly on the features we use (\"Garbage in – garbage out\")\n",
    "* Dimensionality reduction - Reducing the number of variables (features)\n",
    "* * We can do this manually or use algorithms\n",
    "* * Feature selection - Selecting only columns that are useful\n",
    "* * Feature extraction - Getting meaningful features after transformation of raw data (such as non-structured to structured data) \n",
    "* Feature engineering - Using our knowledge of the data to create meaningful features (involves a lot of testing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
