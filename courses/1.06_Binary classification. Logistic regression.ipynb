{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f4e3a47-5274-4503-8f3a-05e7f2a8c38e",
   "metadata": {},
   "source": [
    "* <b>Binary classification</b> is a type of supervised learning problem where the goal is to classify each input, choosing between two possible labels  or classes.\n",
    "* * Example: Class 1 = Spam  |  Class 0 = Not Spam\n",
    "* <b>Logistic regression</b> is a machine learning algorithm used to solve binary classification problems.\n",
    "* * It outputs a probability between 0 and 1, and apply a threshold  to classify the input into one of the two classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2223b-430f-42f2-ae70-988c77042ef3",
   "metadata": {},
   "source": [
    "## Logistic regression model\n",
    "\n",
    "* The outpyt $y$ can take only a small handful of possible values i.e. several \"classes\" or \"categories\".\n",
    "* Fits an an S-shaped curve $g(z)$ to the data, called sigmoid function or logistic regression function.\n",
    "* The output of the sigmoid function is always between 0 and 1.\n",
    "* $g(z)$ = <font size=4>$\\frac{1}{1 + e^{-z}}$</font>, where <font size=2>$0<g(z)<1; e\\approx2.7$</font>\n",
    "* How to model $g(z)$ with  $f_{\\vec{w},b}(\\vec{x}) = \\vec{w}.\\vec{x} + b$ ?\n",
    "* * Substitute $z$ in $g(z)$ with the same old $f_{\\vec{w},b}(\\vec{x})$, already familiar from regression problems.\n",
    "* * Keep on modeling $f_{\\vec{w},b}(\\vec{x})$ with regression gradient descent and cost functions.\n",
    "* * As a result, we can again take as input data the same set of features $\\vec{x}$, but output a number between 0 and 1 for every $x$.\n",
    "* Think of the output of the logistic regression as a \"probability\" for the label $y$ to be true or false (i.e. equal to 1 or 0), given a certain input of $x$.\n",
    "* Example:\n",
    "* * Let $\\vec{x}$ be a feature vector denoting tumor size.\n",
    "* * Let $y$ be a label (class) denoting malignancy: 1 if the tumor is malignant, and 0 if not.\n",
    "* * When we get a prediction value of $y=0.7$ - it means 70% chance for $y$ to be \"true\" (i.e. equal to 1) and therfore for the tumor to be malignant.\n",
    "* A valid notation for logistic regression is also: $f_{\\vec{w},b}(\\vec{x}) = P(y=1 \\mid \\vec{x};\\vec{w},b)$, where $P(y=1) + P(y=0) = 1$\n",
    "* * This litteraly means \"probability that $y$ is 1, given input $\\vec{x}$ and parameters $\\vec{w},b$\n",
    "\n",
    "![sigmoid](1-sig.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db23ed2",
   "metadata": {},
   "source": [
    "## Decision boundary of logistic regression\n",
    "\n",
    "* The decision boundary is the line given by the equation: $z=\\vec{w}.\\vec{x}+b=0$\n",
    "* The model predicts \"true\" when $\\vec{w}.\\vec{x}+b \\geq 0$\n",
    "* Example <b>I</b>:\n",
    "* * $f_{\\vec{w},b}(\\vec{x}) = g(z) = g(w_1x_1 + w_2x_2 + b)$, where $\\vec{w}=(1, 1), b=-3$\n",
    "* * $z = \\vec{w}.\\vec{x}+b = x_1 + x_2 -3 = 0$\n",
    "* * The decision boundry is given by the srtaight line $z: x_1+x_2=3$\n",
    "<br>\n",
    "* The decision boundary can also be different from a straight line.\n",
    "* With a higher-order polynomial terms in $z$, the decision boundary can be a quite complex curve i.e. the logistic regression can fit pretty complex data.\n",
    "* Example <b>II</b>:\n",
    "* * $f_{\\vec{w},b}(\\vec{x}) = g(z) = g(w_1x_1^2 + w_2x_2^2 + b)$, where $\\vec{w}=(1, 1), b=-1$\n",
    "* * $z = \\vec{w}.\\vec{x}+b = x_1^2 + x_2^2 -1 = 0$\n",
    "* * The decision boundry is given by the circle $z: x_1^2+x_2^2=1$\n",
    "\n",
    "![sigmoid](2-sig.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85483f01-20c0-4ce7-aadc-a155e8dd7907",
   "metadata": {},
   "source": [
    "## Cost function for logistic regression\n",
    "\n",
    "* Cost gives us a way to measure how specific parameters fit the training data and to choose better parameters.\n",
    "* The squared error (or mean squared error) cost function is not ideal for logistic regression, as it produces a non-convex shape with lots of local minima, where Gradiesnt descent algorithm can get \"stuck\".\n",
    "* Given by the formula: $L (f_{\\vec{w},b}(\\vec{x}) = \\begin{cases} \n",
    "-\\log (f_{\\vec{w},b}(\\vec{x}) \\; \\mid \\; if \\; y_i = 1 \\\\\n",
    "-\\log (1 - f_{\\vec{w},b}(\\vec{x}) \\; \\mid \\; if \\; y_i = 0 \n",
    "\\end{cases}$\n",
    "\n",
    "![sigmoid](3-sig.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072a6e2-d556-482c-b975-5ec17622fbb4",
   "metadata": {},
   "source": [
    "* Loss function measures how well you're doing on one training example and is by summing up the losses on all of the training examples that you then get, the cost function, which measures how well you're doing on the entire training set.\n",
    "##### For $y_i = 1$:\n",
    "* $\\log(f)$ and $-\\log(f)$ look the same, just flipped along the horizontal axis. Notice that $-\\log(f)$ intersects the horizontal axis at $f=1$ and continues downward from there. Since $f$ is the output of logistic regression - it's always between 0 and 1 i.e. the only part of the function that's relevant is therefore the part between 0 and 1.\n",
    "* If the algorithm predicts a probability close to 1 and the true label is 1, then the loss is very small because we are very close to the right answer. In contrast, if true label is 1 and the prediction is 0.5, the loss is a bit higher, and grows even higher if the algorithm outputs 0.1. Given the \"tumor\" classification example above, if the model thinks that there is only a 10% chance of the tumor being malignant, but y really is 1 (really is malignant) - then the loss is a much higher value.  Overall, the loss function incentivizes or nurtures, or helps push the algorithm to make more accurate predictions because the loss is lowest, when it predicts values close to the true label.\n",
    "##### For $y_i = 0$:\n",
    "* In this case, the loss is $-\\log (1 - f)$ and again the range of $f$ is limited to 0 to 1 because logistic regression only outputs values between 0 and 1.\n",
    "* When $f$ is 0 or very close to 0 and the true label is 0, the loss is also going to be very small - meaning that if the model's prediction is very close to 0, we nearly got it right. The larger the value of $f$ gets, the bigger the loss because the prediction is further from the true label 0. In fact, as that prediction approaches 1, the loss actually approaches infinity. Going back to the tumor classification example, if the model predicts that the patient's tumor is almost certain to be malignant, and it turns out to actually not be malignant, so $y$ equals 0 - then we penalize the model with a very high loss. The further the prediction f of x is away from the true value of y, the higher the loss. If $f$ approaches 0, the loss actually goes really large and in fact approaches infinity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9775ed90-7e36-4fb5-8f69-e3c8c59d6951",
   "metadata": {},
   "source": [
    "## Gradient descent for logistic regression\n",
    "\n",
    "It looks similar to linear regression, but with one significant difference:\n",
    "* Linear regression: $f_{\\vec{w},b}(\\vec{x}) = \\vec{w}.\\vec{x} + b$\n",
    "* Logistic regresion: $g(z) = f_{\\vec{w},b}(\\vec{x}) = $ <font size=4> $\\frac{1}{1 + e^{-(\\vec{w}.\\vec{x} + b)}}$ </font>\n",
    "<br><br>\n",
    "##### Cost:\n",
    "$ J(w,b) = -\\frac{1}{m} \\sum_{i=1}^{m} ( y^{(i)}\\log(f_{\\vec{w},b}(\\vec{x}^{(i)}) ) + ( 1-y^{(i)} ) ( 1 - \\log(f_{\\vec{w},b}(\\vec{x}^{(i)}) ) $\n",
    "<br><br>\n",
    "##### Gradient descent:\n",
    "<i>Repeat until convergence</i> {<br>\n",
    "$ w_j = w_j - \\alpha\\frac{d}{dw} J(\\vec{w},b) $ &emsp; for $j=1 ... n$ (# of features)\n",
    "<br>\n",
    "$ b = b - \\alpha\\frac{d}{db} J(\\vec{w},b)  $\n",
    "<br>} \n",
    "<br><i>where</i>:<br>\n",
    "$\\frac{\\partial}{\\partial w_{j}}J(\\vec{w},b) = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} ( f_{\\vec{w},b}(x^{(i)}) - y^{(i)} )x^{(i)}_{j}$ \n",
    "<br>$\\frac{\\partial}{\\partial b}J(\\vec{w},b) = \\frac{1}{m} \\sum_{i=1}^{m} ( f_{\\vec{w},b}(x^{(i)}) - y^{(i)} )$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a5e2ee-f1c7-463c-b87d-90cce3ae68f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
