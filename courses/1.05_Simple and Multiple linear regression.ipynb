{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4823a15",
   "metadata": {},
   "source": [
    "## Linear Regression  \n",
    "\n",
    "* A <b>supervised</b> learning model (meaning that it learns from labeled data) for predicting a <b>continuous valued output</b> - such as the price of houses (real numbers i.e. scalar values).\n",
    "* Types of Linear regression: \n",
    "* * Univariate (simple) linear regression uses one independent variable to predict the output $\\hat{y}$. \n",
    "* * Multiple linear regression uses two or more independent variables to predict $\\hat{y}$. Using more variables allows the model to account for more factors that influence $y$ and generally improve predictive accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12ed98-3194-414f-9090-f4f6f00715e0",
   "metadata": {},
   "source": [
    "## Simple (Univariate) Linear Regression \n",
    "\n",
    "Simple linear regression model fits a straight line to the data: $f_{w,b}$(x) = $w$x + $b$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db06bf7-c9d1-4857-a6dd-da6aa9f78310",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression \n",
    "\n",
    "Multiple Linear Regression is a model that estimates the relationship between a quantitative dependent variable and two or more independent variables using a straight line: $f_{\\vec{w},b}(\\vec{x}) = \\vec{w}.\\vec{X}$ + $b$ ,   where $.$ is a dot product \n",
    "\n",
    "* $w_{1} ... w_{n}, b = $ weights or coefficients or parameters of the model (adjusted as the model learns from data)\n",
    "* $\\vec{w} = [w_{1} ... w_{n}] = n$-length vector\n",
    "* $b = $ scalar \n",
    "* $\\vec{X} = $ feature matrix with $m$ rows and $n$ columns\n",
    "* $n = $ length of feature (sample) vector\n",
    "* $m = $ number of training samples\n",
    "* $x^{(i)} = (x^{(i)}_{1}, ... , x^{(i)}_{n}) = $ feature vector $i$\n",
    "* $x^{(i)}_{j} = $ element $j$ in sample $i$\n",
    "* $y^{(i)}$ = output or predicted target variable $i$\n",
    "* $(x^{(i)}, y^{(i)})$ = a single training example (the i-th training example) = a single row in a data table\n",
    "* $f_{\\vec{w},b}(x^{(i)}) = w_{1}x_{1}$ + ... + $w_{n}x_{1}$ + $b$\n",
    "* $J(\\vec{w},b) = J(w_{1}...w_{n},b) = \\frac{1}{2m} \\sum_{i=1}^{m} ( f_{\\vec{w},b}(x^{(i)}) - y^{(i)} )^{2} = $ cost function, where $f_{\\vec{w},b}(x^{(i)}) = \\vec{w}.x^{(i)}$ + $b$ = $\\hat{y}^{(i)}$\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Gradient Descent (update rules)\n",
    "\n",
    "Repeat until convergence: {\n",
    "    <br>&nbsp;&nbsp; $w_{j} = w_{j} - \\alpha \\frac{\\partial}{\\partial w_{j}}J(\\vec{w},b)$ \n",
    "    <br>&nbsp;&nbsp;  $b = b - \\alpha \\frac{\\partial}{\\partial b}J(\\vec{w},b)$ <br> \n",
    "}\n",
    "<br><br>\n",
    "where: \n",
    "<br><br>$\\frac{\\partial}{\\partial w_{j}}J(\\vec{w},b) = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} ( f_{\\vec{w},b}(x^{(i)}) - y^{(i)} )x^{(i)}_{j}$ \n",
    "<br><br>$\\frac{\\partial}{\\partial b}J(\\vec{w},b) = \\frac{1}{m} \\sum_{i=1}^{m} ( f_{\\vec{w},b}(x^{(i)}) - y^{(i)} )$\n",
    "\n",
    "### Cost function with regularization:\n",
    "\n",
    "$J(\\vec{w},b)= \\frac{1}{2m} \\sum_{i=1}^{m}[f_{\\vec{w},b}(x^{(i)}) - y^{(i)} ]^{2} + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}w_{j}^2$ \n",
    "\n",
    "###  Gradient descent + regularization (update rules):\n",
    "\n",
    "$ w_{j} = w_{j} - \\alpha [ \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} ( f_{\\vec{w},b}(x^{(i)}) - y^{(i)} ) x^{i}_{j} $ + $ \\frac{\\lambda}{m}\\sum_{j=1}^{n}w_{j} ]$\n",
    "<br><br> \n",
    "$ b = b - \\alpha [\\frac{1}{m} \\sum_{i=1}^{m} ( f_{\\vec{w},b}(x^{(i)}) - y^{(i)} ) ]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c94855-f466-494d-931d-31a554ba816d",
   "metadata": {},
   "source": [
    "## Alternative to Gradient Descent:\n",
    " \n",
    "> For models like Linear Regression, we can use two types of techniques to fit the parameter: Normal Equation and Gradient descent. \n",
    "\n",
    "* Normal equation is used only for linear regression i.e. does not generalize for other learning algorithms\n",
    "* It's slow for number of features > 10000\n",
    "* May be used on the background by ML libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac31ac96-6cef-458a-918d-4317e3832e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
